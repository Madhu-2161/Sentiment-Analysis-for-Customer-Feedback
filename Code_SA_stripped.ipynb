{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import string\n",
    "string.punctuation\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/bank_reviews3.csv', encoding='ISO-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['author','bank_image','review_title_by_user','rating_title_by_user'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"review\"].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['Date','Address','Bank','Rating', 'Review','Useful_Count']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_to_sentiment(r):\n",
    "    if r >= 4:\n",
    "        return 1\n",
    "    elif r == 3:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "df['Polarity'] = df['Rating'].apply(rating_to_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts=df.Polarity.value_counts()\n",
    "value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts.plot(kind=\"bar\", colormap='viridis')\n",
    "plt.xlabel(\"Polarity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Polarity Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n",
    "df.info()\n",
    "\n",
    "df_og = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(text):\n",
    "    words = text.split()\n",
    "    lower = [word.lower() for word in words]\n",
    "    return ' '.join(lower)\n",
    "\n",
    "df['Review']= df['Review'].apply(lambda x:lower(x))\n",
    "\n",
    "print(\"Converted all the reviews to lower case \")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_removed = 0\n",
    "\n",
    "def hyperlinks(text):\n",
    "    global link_removed\n",
    "    pattern = r'http\\S+|www\\S+'\n",
    "    matches = re.findall(pattern , text)\n",
    "    link_removed += len(matches)\n",
    "    removed = re.sub(pattern, '', text)\n",
    "    return removed\n",
    "\n",
    "df['Review']= df['Review'].apply(lambda x:hyperlinks(x))\n",
    "print(f\"successfully removed {link_removed} hyperlinks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_spaces = 0\n",
    "def remove_large_spaces(text):\n",
    "    global large_spaces\n",
    "    pattern = r'\\s{2,}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    large_spaces += len(matches)\n",
    "    removed_spaces = re.sub(pattern, ' ', text)\n",
    "    return removed_spaces.strip()\n",
    "df['Review']= df['Review'].apply(lambda x:remove_large_spaces(x))\n",
    "print(f\"successfully removed {large_spaces} large spaces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = set(nltk.corpus.stopwords.words('english'))\n",
    "print(\"total stopwords : \", len(stopword))\n",
    "print(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_count = 0\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    global stopword_count\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stopword]\n",
    "    stopword_count += len(words) - len(filtered_words)\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "df['Review'] = df['Review'].astype(str).apply(remove_stopwords)\n",
    "\n",
    "print(f\"Successfully removed {stopword_count} stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_count = 0\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    global punctuation_count\n",
    "    punctuationfree = []\n",
    "    for char in text:\n",
    "        if char in string.punctuation:\n",
    "            punctuation_count += 1\n",
    "        else:\n",
    "            punctuationfree.append(char)\n",
    "    return ''.join(punctuationfree)\n",
    "\n",
    "\n",
    "df['Review'] = df['Review'].astype(str).apply(remove_punctuation)\n",
    "print(f\"Successfully removed {punctuation_count} punctuation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "number_count = 0\n",
    "\n",
    "def remove_numbers(text):\n",
    "    global number_count\n",
    "    matches = re.findall(r'\\d+', text)\n",
    "    number_count += sum(len(match) for match in matches)\n",
    "    removed_numbers = re.sub(r'\\d+', '', text)\n",
    "    return removed_numbers\n",
    "\n",
    "df['Review'] = df['Review'].astype(str).apply(remove_numbers)\n",
    "\n",
    "print(f\"Successfully removed numbers. Total digits removed: {number_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html_re = re.compile(r'<.*?>')\n",
    "    text = re.sub(html_re, '', text)\n",
    "    return text\n",
    "df['Review']= df['Review'].apply(lambda x:remove_html(x))\n",
    "print(\"successfully removed html tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_count = 0\n",
    "time_count = 0\n",
    "\n",
    "def remove_date_time(text):\n",
    "    global date_count, time_count\n",
    "\n",
    "    # Match MM/DD/YYYY or MM/DD/YY\n",
    "    date_pattern = r\"\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b\"\n",
    "    # Match HH:MM or HH:MMAM / HH:MMPM (optional AM/PM)\n",
    "    time_pattern = r\"\\b\\d{1,2}:\\d{2}(?:[AP]M)?\\b\"\n",
    "\n",
    "    dates = re.findall(date_pattern, text)\n",
    "    times = re.findall(time_pattern, text)\n",
    "    date_count += len(dates)\n",
    "    time_count += len(times)\n",
    "\n",
    "    text_without_date = re.sub(date_pattern, '', text)\n",
    "    text_without_date_time = re.sub(time_pattern, '', text_without_date)\n",
    "    return text_without_date_time.strip()\n",
    "\n",
    "df['Review'] = df['Review'].astype(str).apply(remove_date_time)\n",
    "\n",
    "print(f\"Successfully removed date and time.\")\n",
    "print(f\"Total date patterns removed: {date_count}\")\n",
    "print(f\"Total time patterns removed: {time_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_count = 0\n",
    "hashtag_count = 0\n",
    "\n",
    "def remove_mentions_hashtags(text):\n",
    "    global mention_count, hashtag_count\n",
    "\n",
    "    mentions = re.findall(r\"@\\w+\", text)\n",
    "    mention_count += len(mentions)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    hashtags = re.findall(r\"#\\w+\", text)\n",
    "    hashtag_count += len(hashtags)\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "df['Review'] = df['Review'].astype(str).apply(remove_mentions_hashtags)\n",
    "\n",
    "print(\"Successfully removed mentions and hashtags.\")\n",
    "print(f\"Mentions removed: {mention_count}\")\n",
    "print(f\"Hashtags removed: {hashtag_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_word_changes = 0\n",
    "\n",
    "def stem_text(text):\n",
    "    global stemmed_word_changes\n",
    "    stemmer = PorterStemmer()\n",
    "    words = text.split()\n",
    "    stemmed_words = []\n",
    "\n",
    "    for word in words:\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        if stemmed_word != word:\n",
    "            stemmed_word_changes += 1\n",
    "        stemmed_words.append(stemmed_word)\n",
    "\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "df['Review'] = df['Review'].astype(str).apply(stem_text)\n",
    "\n",
    "print(\"Successfully stemmed the text.\")\n",
    "print(f\"Total words changed after stemming: {stemmed_word_changes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data):\n",
    "    tokens = word_tokenize(data)\n",
    "    return tokens\n",
    "\n",
    "df['Review']= df['Review'].apply(lambda x:tokenize_data(x))\n",
    "print(\"successfully tokenized the text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text statistics\n",
    "df['comment_length'] = df['Review'].dropna().astype(str).apply(len)\n",
    "df['word_count'] = df['Review'].dropna().astype(str).apply(lambda x: len(x.split()))\n",
    "df['unique_word_count'] = df['Review'].dropna().astype(str).apply(lambda x: len(set(x.split())))\n",
    "\n",
    "# Histograms\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "# Comment length\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(df['comment_length'], bins=30, color='skyblue', kde=True)\n",
    "plt.title('Distribution of Comment Length')\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Word count\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(df['word_count'], bins=30, color='lightgreen', kde=True)\n",
    "plt.title('Distribution of Word Count')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Unique word count\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(df['unique_word_count'], bins=30, color='salmon', kde=True)\n",
    "plt.title('Distribution of Unique Word Count')\n",
    "plt.xlabel('Unique Words')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots: comment stats vs. rating\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.boxplot(x=df['Rating'], y=df['comment_length'], palette='Blues')\n",
    "plt.title('Comment Length by Rating')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Length')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.boxplot(x=df['Rating'], y=df['word_count'], palette='Greens')\n",
    "plt.title('Word Count by Rating')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Words')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.boxplot(x=df['Rating'], y=df['unique_word_count'], palette='Reds')\n",
    "plt.title('Unique Word Count by Rating')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Unique Words')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(6, 4))\n",
    "corr = df[['comment_length', 'word_count', 'unique_word_count']].corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-rating average stats\n",
    "rating_stats = df.groupby('Rating')[['comment_length', 'word_count', 'unique_word_count']].mean().round(1)\n",
    "print(\"Average Comment Stats by Rating:\")\n",
    "rating_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(df['Rating'], bins=10, kde=True, color='blue')\n",
    "plt.title('Distribution of Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='Rating', data=df, palette='pastel')\n",
    "plt.title('Number of Reviews per Rating')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year_month'] = df['Date'].dt.to_period('M')\n",
    "\n",
    "monthly_sentiment_counts = df.groupby(['year_month', 'Polarity']).size().unstack(fill_value=0)\n",
    "\n",
    "print(monthly_sentiment_counts.tail())\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "monthly_sentiment_counts.plot(kind='line', marker='o', figsize=(12, 6))\n",
    "\n",
    "plt.title('Monthly Sentiment Trends in Customer Reviews')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.legend(title='Sentiment')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_sentiment_counts = df.groupby(['Address', 'Polarity']).size().unstack(fill_value=0)\n",
    "\n",
    "print(city_sentiment_counts)\n",
    "\n",
    "# Stacked bar chart\n",
    "city_sentiment_counts.plot(kind='bar', stacked=True, figsize=(40, 20), colormap='Set2')\n",
    "plt.title('City-wise Sentiment Distribution of Reviews')\n",
    "plt.xlabel('City')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = df['Review'].dropna().astype(str)\n",
    "\n",
    "text_all = \" \".join(comments)\n",
    "\n",
    "# Word Cloud for Unigrams\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords='english').generate(text_all)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud - Unigrams')\n",
    "plt.show()\n",
    "\n",
    "# Bar Chart - Top 20 Unigrams\n",
    "vectorizer_uni = CountVectorizer(stop_words='english', max_features=20)\n",
    "X_uni = vectorizer_uni.fit_transform(comments)\n",
    "words = vectorizer_uni.get_feature_names_out()\n",
    "counts = X_uni.sum(axis=0).A1\n",
    "df_uni = pd.DataFrame({'word': words, 'frequency': counts}).sort_values(by='frequency', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df_uni, x='frequency', y='word', palette='magma')\n",
    "plt.title('Top 20 Unigrams')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Word')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud for Bigrams\n",
    "vectorizer_bi = CountVectorizer(ngram_range=(2, 2), stop_words='english', max_features=100)\n",
    "X_bi = vectorizer_bi.fit_transform(comments)\n",
    "bigrams = vectorizer_bi.get_feature_names_out()\n",
    "counts_bi = X_bi.sum(axis=0).A1\n",
    "text_bigrams = dict(zip(bigrams, counts_bi))\n",
    "wordcloud_bi = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(text_bigrams)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud_bi, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud - Bigrams')\n",
    "plt.show()\n",
    "\n",
    "# Bar Chart - Top 20 Bigrams\n",
    "df_bi = pd.DataFrame({'bigram': bigrams, 'frequency': counts_bi}).sort_values(by='frequency', ascending=False).head(20)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df_bi, x='frequency', y='bigram', palette='magma')\n",
    "plt.title('Top 20 Bigrams')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Bigram')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', max_features=20)\n",
    "X = vectorizer.fit_transform(df['Review'].dropna().astype(str))\n",
    "word_freq = X.sum(axis=0).A1\n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "freq_df = pd.DataFrame({'word': words, 'frequency': word_freq})\n",
    "freq_df = freq_df.sort_values(by='frequency', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=freq_df, x='frequency', y='word', palette='viridis')\n",
    "plt.title('Top 20 Most Frequent Words in Comments')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Word')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow_vectorizer = CountVectorizer(stop_words='english', max_features=100)\n",
    "X_bow = bow_vectorizer.fit_transform(df['Review'].astype(str))\n",
    "\n",
    "print(f\"BoW Shape: {X_bow.shape}\")\n",
    "print(\"BoW - Top 10 Features:\")\n",
    "print(bow_vectorizer.get_feature_names_out()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df['Review'] = df['Review'].astype(str)\n",
    "\n",
    "# TF-IDF Vectorization with unigrams (1-grams)\n",
    "vectorizer_unigram = TfidfVectorizer(ngram_range=(1, 1), stop_words='english', max_features=100)\n",
    "X_unigram = vectorizer_unigram.fit_transform(df['Review'])\n",
    "\n",
    "print(\"Top 10 Unigram Features:\")\n",
    "print(vectorizer_unigram.get_feature_names_out()[:10])\n",
    "\n",
    "# TF-IDF Vectorization with bigrams (2-grams)\n",
    "vectorizer_bigram = TfidfVectorizer(ngram_range=(2, 2), stop_words='english', max_features=100)\n",
    "X_bigram = vectorizer_bigram.fit_transform(df['Review'])\n",
    "\n",
    "print(\"\\nTop 10 Bigram Features:\")\n",
    "print(vectorizer_bigram.get_feature_names_out()[:10])\n",
    "\n",
    "# TF-IDF Vectorization with both unigrams and bigrams (1-2 grams)\n",
    "vectorizer_1_2gram = TfidfVectorizer(ngram_range=(1, 2), stop_words='english', max_features=100)\n",
    "X_1_2gram = vectorizer_1_2gram.fit_transform(df['Review'])\n",
    "\n",
    "print(f\"\\nTF-IDF Shape: {X_1_2gram.shape}\")\n",
    "print(\"Top 10 1-2 Gram Features:\")\n",
    "print(vectorizer_1_2gram.get_feature_names_out()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec Embeddings with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and tokenize the reviews\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_reviews = df['Review'].apply(preprocess_text)\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(sentences=tokenized_reviews,\n",
    "                     vector_size=100,\n",
    "                     window=5,\n",
    "                     min_count=2,\n",
    "                     workers=4,\n",
    "                     sg=1)\n",
    "\n",
    "word_vec = w2v_model.wv['account']\n",
    "print(\"Vector for 'account':\", word_vec[:10])\n",
    "\n",
    "print(\"\\nTop 5 words similar to 'loan':\")\n",
    "w2v_model.wv.most_similar('loan', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_vector(tokens, model):\n",
    "    vec = np.zeros(model.vector_size)\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        if word in model.wv:\n",
    "            vec += model.wv[word]\n",
    "            count += 1\n",
    "    return vec / count if count != 0 else vec\n",
    "\n",
    "\n",
    "review_vectors = tokenized_reviews.apply(lambda x: get_avg_vector(x, w2v_model))\n",
    "review_vectors_matrix = np.vstack(review_vectors.values)\n",
    "\n",
    "print(\"\\nShape of Word2Vec review matrix:\", review_vectors_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "def get_avg_glove_vector(text):\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)\n",
    "    words = word_tokenize(text.lower())\n",
    "    vectors = [glove[w] for w in words if w in glove]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(100)\n",
    "\n",
    "embeddings = df['Review'].fillna('').apply(get_avg_glove_vector)\n",
    "X_glove = np.vstack(embeddings.values)\n",
    "\n",
    "print(f\"GloVe Embedding Shape: {X_glove.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  VADER (Valence Aware Dictionary for Sentiment Reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "df_og['vader_sentiment'] = df_og['Review'].apply(lambda x: vader.polarity_scores(str(x))['compound'])\n",
    "\n",
    "# Label sentiment\n",
    "df_og['vader_label'] = df_og['vader_sentiment'].apply(\n",
    "    lambda x: 'positive' if x > 0.05 else 'negative' if x < -0.05 else 'neutral'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_og['vader_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "df_og['textblob_polarity'] = df_og['Review'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
    "df_og['textblob_subjectivity'] = df_og['Review'].apply(lambda x: TextBlob(str(x)).sentiment.subjectivity)\n",
    "\n",
    "df_og['textblob_label'] = df_og['textblob_polarity'].apply(\n",
    "    lambda x: 'positive' if x > 0.1 else 'negative' if x < -0.1 else 'neutral'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_og['textblob_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spaCy (via TextBlob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def spacy_textblob_sentiment(text):\n",
    "    doc = nlp(str(text))\n",
    "    blob = TextBlob(doc.text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "df_og['spacy_textblob_polarity'] = df_og['Review'].apply(spacy_textblob_sentiment)\n",
    "\n",
    "df_og['spacy_label'] = df_og['spacy_textblob_polarity'].apply(\n",
    "    lambda x: 'positive' if x > 0.1 else 'negative' if x < -0.1 else 'neutral'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_og['spacy_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of VADER, TextBlob and spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df_og[['Review', 'vader_label', 'textblob_label', 'spacy_label']].sample(10, random_state=42)\n",
    "print(\"Sample Sentiment Comparison:\\n\")\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSentiment Distribution:\")\n",
    "print(\"\\nVADER:\\n\", df_og['vader_label'].value_counts())\n",
    "print(\"\\nTextBlob:\\n\", df_og['textblob_label'].value_counts())\n",
    "print(\"\\nspaCy+TextBlob:\\n\", df_og['spacy_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_sentiment(score):\n",
    "    if score > 0.1:\n",
    "        return 'Positive'\n",
    "    elif score < -0.1:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_og['vader_sentiment'] = df_og['vader_sentiment'].apply(categorize_sentiment)\n",
    "df_og['textblob_polarity'] = df_og['textblob_polarity'].apply(categorize_sentiment)\n",
    "df_og['spacy_textblob_polarity'] = df_og['spacy_textblob_polarity'].apply(categorize_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = df_og[['vader_sentiment', 'textblob_polarity', 'spacy_textblob_polarity']]\n",
    "label_counts = label_df.apply(pd.Series.value_counts).T.fillna(0)\n",
    "\n",
    "label_counts.plot(kind='bar', stacked=True, figsize=(8, 5), colormap='Set2')\n",
    "plt.title('Sentiment Label Distribution Across Tools')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.xlabel('Sentiment Tool')\n",
    "plt.legend(title='Sentiment')\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_og['vader_score'] = df_og['Review'].apply(lambda x: vader.polarity_scores(x)['compound'])\n",
    "df_og['textblob_score'] = df_og['Review'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "df_og['spacy_score'] = df_og['Review'].apply(lambda x: TextBlob(nlp(x).text).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(score, pos_thres=0.1, neg_thres=-0.1):\n",
    "    if score >= pos_thres:\n",
    "        return 'positive'\n",
    "    elif score <= neg_thres:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "df_og['vader_label'] = df_og['vader_score'].apply(get_label)\n",
    "df_og['textblob_label'] = df_og['textblob_score'].apply(get_label)\n",
    "df_og['spacy_label'] = df_og['spacy_score'].apply(get_label)\n",
    "\n",
    "# Identify disagreement cases\n",
    "df_og['disagreement'] = df_og.apply(\n",
    "    lambda row: len(set([row['vader_label'], row['textblob_label'], row['spacy_label']])) > 1,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Extract and view disagreement samples\n",
    "disagreements = df_og[df_og['disagreement'] == True]\n",
    "print(\"Number of disagreements:\", len(disagreements))\n",
    "\n",
    "sample_disagreements = disagreements[['Review', 'vader_label', 'textblob_label', 'spacy_label']].sample(5, random_state=42)\n",
    "sample_disagreements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame({\n",
    "    'Agree': [len(df) - len(disagreements)],\n",
    "    'Disagree': [len(disagreements)]\n",
    "})\n",
    "print(\"\\nAgreement Summary:\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import ast\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression and Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'] = df['Review'].apply(\n",
    "    lambda x: ' '.join(ast.literal_eval(x)) if isinstance(x, str) and x.startswith('[') else x\n",
    ")\n",
    "\n",
    "\n",
    "X = df['Review']\n",
    "y = df['Polarity']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english', max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_bal, y_train_bal = smote.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "# Train\n",
    "log_reg = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "log_reg.fit(X_train_tfidf, y_train)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tfidf, y_train)\n",
    "\n",
    "\n",
    "# Predict\n",
    "y_pred_log = log_reg.predict(X_test_tfidf)\n",
    "y_pred_nb = nb.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(classification_report(y_test, y_pred_log))\n",
    "\n",
    "print(\"\\nNaive Bayes Results:\")\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Confusion matrix for logistic regression\n",
    "cm = confusion_matrix(y_test, y_pred_log)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix for naive bayes\n",
    "cm = confusion_matrix(y_test, y_pred_nb)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix -Naive Bayes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels: -1 → 0, 0 → 1, 1 → 2\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "\n",
    "# Random oversampling on raw text\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(np.array(X).reshape(-1, 1), y_enc)\n",
    "X_resampled = X_resampled.flatten()\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train_enc, y_test_enc = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.2, stratify=y_resampled, random_state=42\n",
    ")\n",
    "\n",
    "# One-hot encode targets\n",
    "y_train_cat = to_categorical(y_train_enc)\n",
    "y_test_cat = to_categorical(y_test_enc)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Padding\n",
    "max_len = 100\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "\n",
    "# Train Word2Vec embeddings on tokenized training data\n",
    "tokenized = [word_tokenize(text.lower()) for text in X_train]\n",
    "w2v_model = Word2Vec(sentences=tokenized, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_dim = 100\n",
    "vocab_size = 10000\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < vocab_size and word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_dim,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=max_len,\n",
    "    trainable=True\n",
    "))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile with lower learning rate\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate=5e-4),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_pad,\n",
    "    y_train_cat,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_pad, y_test_cat),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "y_pred = model.predict(X_test_pad)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "print(classification_report(y_true_labels, y_pred_labels, target_names=le.classes_.astype(str)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model.predict(X_test_pad)\n",
    "\n",
    "# Convert probabilities to predicted classes\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# True labels (already encoded to 0, 1, 2)\n",
    "y_true = y_test_enc\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_.astype(str)))\n",
    "\n",
    "# Accuracy Score\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels from 'Polarity'\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['Polarity'])\n",
    "\n",
    "# Apply RandomOverSampler on raw reviews\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "text_resampled, label_resampled = ros.fit_resample(\n",
    "    np.array(df['Review']).reshape(-1, 1), df['label']\n",
    ")\n",
    "\n",
    "# Create a balanced DataFrame\n",
    "df_balanced = pd.DataFrame({'Review': text_resampled.flatten(), 'label': label_resampled})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['Polarity'])\n",
    "\n",
    "# Train-test split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df_balanced['Review'].tolist(), df_balanced['label'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load tokenizer and encode data\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Convert to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_dict({**train_encodings, 'label': train_labels})\n",
    "val_dataset = Dataset.from_dict({**val_encodings, 'label': val_labels})\n",
    "\n",
    "# Load pre-trained model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "eval_result = trainer.evaluate()\n",
    "print(\"\\nEvaluation Results:\", eval_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_output = trainer.predict(val_dataset)\n",
    "preds = np.argmax(preds_output.predictions, axis=1)\n",
    "\n",
    "print(\"DistilBERT:\\n\", classification_report(val_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "cm = confusion_matrix(val_labels, preds)\n",
    "labels = np.unique(np.concatenate([val_labels, preds]))\n",
    "\n",
    "# Display confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix: DistilBERT\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(y_true, y_pred, model_name):\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'accuracy': report['accuracy'],\n",
    "        'f1_positive': report['1']['f1-score'],\n",
    "        'f1_negative': report['0']['f1-score']\n",
    "    }\n",
    "\n",
    "results = []\n",
    "results.append(get_scores(y_test, y_pred_log, 'Logistic Regression'))\n",
    "results.append(get_scores(y_test, y_pred_nb, 'Naive Bayes'))\n",
    "results.append(get_scores(y_true_labels, y_pred_labels, 'LSTM'))\n",
    "results.append(get_scores(val_labels, preds, 'DistilBERT'))\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Accuracy Comparison\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x='model', y='accuracy', data=results_df, palette='viridis')\n",
    "plt.title(\"Model Accuracy Comparison\")\n",
    "plt.ylim(0.5, 1.0)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# F1-Score per Class\n",
    "f1_df = results_df.melt(id_vars='model', value_vars=['f1_positive', 'f1_negative'], var_name='class', value_name='f1_score')\n",
    "f1_df['class'] = f1_df['class'].map({'f1_positive': 'Positive', 'f1_negative': 'Negative'})\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "sns.barplot(x='model', y='f1_score', hue='class', data=f1_df, palette='mako')\n",
    "plt.title(\"F1-Score by Sentiment Class\")\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.legend(title='Class')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top complaint themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(r):\n",
    "    if r >= 4:\n",
    "        return 'positive'\n",
    "    elif r <= 2:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df['Polarity'] = df['Rating'].apply(get_sentiment)\n",
    "df = df[df['Polarity'] == 'negative']\n",
    "\n",
    "# Clean and tokenize\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return [word for word in tokens if word not in stopwords.words('english') and len(word) > 2]\n",
    "\n",
    "df['tokens'] = df['Review'].apply(preprocess)\n",
    "\n",
    "# Flatten all tokens into a single list\n",
    "all_tokens = [word for tokens in df['tokens'] for word in tokens]\n",
    "\n",
    "# Get most common words\n",
    "word_counts = Counter(all_tokens)\n",
    "top_words = word_counts.most_common(20)\n",
    "\n",
    "# Convert to DataFrame for plotting\n",
    "top_df = pd.DataFrame(top_words, columns=['word', 'count'])\n",
    "\n",
    "# Bar Plot of Top Keywords\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='count', y='word', data=top_df, palette='rocket')\n",
    "plt.title('Top Complaint Keywords in Negative Reviews')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Keyword')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Word Cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_counts)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Word Cloud of Complaint Themes\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bigram Analysis\n",
    "bigrams = Counter(ngrams(all_tokens, 2))\n",
    "top_bigrams = bigrams.most_common(10)\n",
    "print(\"\\nTop Complaint Bigrams:\")\n",
    "for phrase, count in top_bigrams:\n",
    "    print(f\"{' '.join(phrase)}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect Based Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_sentiment(r):\n",
    "    if r >= 4:\n",
    "        return 'positive'\n",
    "    elif r <= 2:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df['Polarity'] = df['Rating'].apply(get_sentiment)\n",
    "\n",
    "# Define aspect keywords (you can expand this)\n",
    "aspect_keywords = ['service', 'loan', 'card', 'charges', 'staff', 'app', 'transaction', 'support']\n",
    "\n",
    "# ABSA function\n",
    "def extract_aspects(text):\n",
    "    doc = nlp(text)\n",
    "    aspects = []\n",
    "    for token in doc:\n",
    "        if token.text in aspect_keywords:\n",
    "            # Find nearby adjective or opinion\n",
    "            window = doc[max(token.i - 3, 0): min(token.i + 4, len(doc))]\n",
    "            sentiment_phrase = window.text\n",
    "            polarity = TextBlob(sentiment_phrase).sentiment.polarity\n",
    "            label = 'positive' if polarity > 0.1 else 'negative' if polarity < -0.1 else 'neutral'\n",
    "            aspects.append({'aspect': token.text, 'opinion': sentiment_phrase, 'Polarity': label})\n",
    "    return aspects\n",
    "\n",
    "# Apply to a sample (or full dataset)\n",
    "sample_reviews = df.sample(20, random_state=42)['Review']\n",
    "all_aspects = []\n",
    "\n",
    "for review in sample_reviews:\n",
    "    aspects = extract_aspects(review)\n",
    "    for asp in aspects:\n",
    "        asp['Review'] = review\n",
    "        all_aspects.append(asp)\n",
    "\n",
    "# Convert to DataFrame\n",
    "aspects_df = pd.DataFrame(all_aspects)\n",
    "print(\"Extracted Aspect-Based Sentiment:\")\n",
    "aspects_df[['Review', 'aspect', 'opinion', 'Polarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count by aspect\n",
    "print(\"\\nAspect Sentiment Counts:\")\n",
    "aspects_df.groupby(['aspect', 'Polarity']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Department level insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "department_keywords = {\n",
    "    \"loan process\": [\"loan\", \"loan process\", \"home loan\", \"personal loan\"],\n",
    "    \"branch experience\": [\"branch\", \"staff\", \"manager\", \"in person\"],\n",
    "    \"customer service\": [\"customer service\", \"support\", \"call\", \"response\"],\n",
    "    \"account opening\": [\"account opening\", \"new account\", \"open account\"],\n",
    "    \"mobile app\": [\"app\", \"application\", \"mobile\", \"online\"],\n",
    "    \"charges/fees\": [\"charges\", \"fees\", \"hidden charge\", \"deduction\"],\n",
    "    \"credit card\": [\"credit card\", \"card limit\", \"card issue\"]\n",
    "}\n",
    "\n",
    "# Function to match and score sentiment\n",
    "def extract_department_sentiments(text):\n",
    "    sentiments = []\n",
    "    for dept, keywords in department_keywords.items():\n",
    "        for kw in keywords:\n",
    "            if re.search(rf'\\b{re.escape(kw)}\\b', text):\n",
    "                sentiment_score = TextBlob(text).sentiment.polarity\n",
    "                label = 'positive' if sentiment_score > 0.1 else 'negative' if sentiment_score < -0.1 else 'neutral'\n",
    "                sentiments.append((dept, kw, sentiment_score, label))\n",
    "    return sentiments\n",
    "\n",
    "# Apply to dataset\n",
    "results = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    review = row['Review']\n",
    "    matches = extract_department_sentiments(review)\n",
    "    for dept, kw, score, label in matches:\n",
    "        results.append({\n",
    "            \"department\": dept,\n",
    "            \"matched_phrase\": kw,\n",
    "            \"Review\": review,\n",
    "            \"sentiment_score\": score,\n",
    "            \"Polarity\": label\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "insights_df = pd.DataFrame(results)\n",
    "\n",
    "# View sample insights\n",
    "print(\"Sample Department-Level Insights:\")\n",
    "insights_df.head(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment counts per department\n",
    "summary = insights_df.groupby(['department', 'Polarity']).size().unstack(fill_value=0)\n",
    "print(\"\\nDepartment-Level Sentiment Summary:\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked Bar Chart: Sentiment Count per Department\n",
    "summary = insights_df.groupby(['department', 'Polarity']).size().unstack(fill_value=0)\n",
    "summary = summary[['positive', 'neutral', 'negative']]  # Consistent order\n",
    "\n",
    "summary.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='Set2')\n",
    "plt.title('Department-Level Sentiment Distribution')\n",
    "plt.xlabel('Department / Service')\n",
    "plt.ylabel('Number of Mentions')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Sentiment')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Pie Chart for Each Department\n",
    "for dept in summary.index:\n",
    "    sentiment_counts = summary.loc[dept]\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=140, colors=['green', 'gray', 'red'])\n",
    "    plt.title(f'Sentiment Distribution for \"{dept}\"')\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
